{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN+VancEEOM7hWuotZ0SKgm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lengtsp/llamafactory_V2024-07-04_changeconfig/blob/main/LLamaFactory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. เตรียม train gradio ในเบื้องต้น\n",
        "https://medium.com/engineered-publicis-sapient/master-fine-tuning-in-just-30-minutes-elevate-your-ai-skills-in-just-30-minutes-with-an-engaging-4b0481805881"
      ],
      "metadata": {
        "id": "R7wnVehY5j1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. data > dataset_info.json\n",
        "เพิ่มอันนี้ลงไป\n",
        "\n",
        "  \"airesearch/WangchanThaiInstruct_7.24\" : {\n",
        "        \"hf_hub_url\":\"airesearch/WangchanThaiInstruct_7.24\",\n",
        "        \"columns\": {\n",
        "            \"prompt\":\"Instruction\",\n",
        "            \"query\":\"Input\",\n",
        "            \"response\":\"Output\"\n",
        "        }\n",
        "  },\n",
        "\n",
        "\n",
        "2. webui.py แก้ share = true\n"
      ],
      "metadata": {
        "id": "kSY4nGR_54A-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "\n",
        "\n",
        "--depth 1 ในคำสั่ง git clone หมายถึงการโคลน (clone) repository โดยดึงเฉพาะ commit ล่าสุดเท่านั้น ไม่ดึงประวัติทั้งหมดของ repository นั้น"
      ],
      "metadata": {
        "id": "oXHVtjhT6OwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
        "\n",
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaFAU1d01KB1",
        "outputId": "78050827-0e44-447a-f382-dfe932884bed"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 287, done.\u001b[K\n",
            "remote: Counting objects: 100% (287/287), done.\u001b[K\n",
            "remote: Compressing objects: 100% (247/247), done.\u001b[K\n",
            "remote: Total 287 (delta 73), reused 125 (delta 28), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (287/287), 7.83 MiB | 26.20 MiB/s, done.\n",
            "Resolving deltas: 100% (73/73), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd LLaMA-Factory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcnSvuTI1ONQ",
        "outputId": "15b83d1a-c80f-4e04-9a9e-70977cccdb72"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e \".[torch,metrics]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2o9r-0qNB9K",
        "outputId": "ec752e1c-25ab-49c1-f4ca-78e94082de67"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers>=4.41.2 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (4.41.2)\n",
            "Collecting datasets>=2.16.0 (from llamafactory==0.8.3.dev0)\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate>=0.30.1 (from llamafactory==0.8.3.dev0)\n",
            "  Downloading accelerate-0.32.0-py3-none-any.whl (314 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.0/314.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft>=0.11.1 (from llamafactory==0.8.3.dev0)\n",
            "  Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trl>=0.8.6 (from llamafactory==0.8.3.dev0)\n",
            "  Downloading trl-0.9.4-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio>=4.0.0 (from llamafactory==0.8.3.dev0)\n",
            "  Downloading gradio-4.37.2-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (2.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (1.11.4)\n",
            "Collecting einops (from llamafactory==0.8.3.dev0)\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (0.1.99)\n",
            "Collecting tiktoken (from llamafactory==0.8.3.dev0)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (3.20.3)\n",
            "Collecting uvicorn (from llamafactory==0.8.3.dev0)\n",
            "  Downloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (2.8.0)\n",
            "Collecting fastapi (from llamafactory==0.8.3.dev0)\n",
            "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sse-starlette (from llamafactory==0.8.3.dev0)\n",
            "  Downloading sse_starlette-2.1.2-py3-none-any.whl (9.3 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (3.7.1)\n",
            "Collecting fire (from llamafactory==0.8.3.dev0)\n",
            "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (24.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (6.0.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (2.3.0+cu121)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (3.8.1)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.3.dev0) (0.42.1)\n",
            "Collecting rouge-chinese (from llamafactory==0.8.3.dev0)\n",
            "  Downloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.30.1->llamafactory==0.8.3.dev0) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.30.1->llamafactory==0.8.3.dev0) (0.23.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.30.1->llamafactory==0.8.3.dev0) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (3.15.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets>=2.16.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.16.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests>=2.32.2 (from datasets>=2.16.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (4.66.4)\n",
            "Collecting xxhash (from datasets>=2.16.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets>=2.16.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (3.9.5)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (4.2.2)\n",
            "Collecting ffmpy (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==1.0.2 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading gradio_client-1.0.2-py3-none-any.whl (318 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.2/318.2 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx>=0.24.1 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (2.1.5)\n",
            "Collecting orjson~=3.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (9.4.0)\n",
            "Collecting pydub (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Collecting ruff>=0.2.2 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading ruff-0.5.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.12.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (2.0.7)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==1.0.2->gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.8.3.dev0) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.8.3.dev0) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.8.3.dev0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.8.3.dev0) (2.20.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.3.dev0) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.3.dev0) (3.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.3.dev0) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1->llamafactory==0.8.3.dev0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.41.2->llamafactory==0.8.3.dev0) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.41.2->llamafactory==0.8.3.dev0) (0.19.1)\n",
            "Collecting tyro>=0.5.11 (from trl>=0.8.6->llamafactory==0.8.3.dev0)\n",
            "  Downloading tyro-0.8.5-py3-none-any.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.4/103.4 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.8.3.dev0) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn->llamafactory==0.8.3.dev0)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.38.0,>=0.37.2 (from fastapi->llamafactory==0.8.3.dev0)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi->llamafactory==0.8.3.dev0)\n",
            "  Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi->llamafactory==0.8.3.dev0)\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi->llamafactory==0.8.3.dev0)\n",
            "  Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.8.3.dev0) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.8.3.dev0) (2.4.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->llamafactory==0.8.3.dev0) (1.4.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from sse-starlette->llamafactory==0.8.3.dev0) (3.7.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.12.1)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->llamafactory==0.8.3.dev0)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi->llamafactory==0.8.3.dev0) (3.7)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0) (4.0.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.3.dev0) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.3.dev0)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.3.dev0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->llamafactory==0.8.3.dev0) (3.3.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->sse-starlette->llamafactory==0.8.3.dev0) (1.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (13.7.1)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl>=0.8.6->llamafactory==0.8.3.dev0) (0.16)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl>=0.8.6->llamafactory==0.8.3.dev0)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Collecting httptools>=0.5.0 (from uvicorn->llamafactory==0.8.3.dev0)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn->llamafactory==0.8.3.dev0)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn->llamafactory==0.8.3.dev0)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn->llamafactory==0.8.3.dev0)\n",
            "  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.1->llamafactory==0.8.3.dev0) (1.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.18.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.1.2)\n",
            "Building wheels for collected packages: fire, llamafactory, ffmpy\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117029 sha256=da5021c8bd1c6089aba9523060a2d1d71c7b86266796ef0748a4119bc17edb06\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.8.3.dev0-0.editable-py3-none-any.whl size=20620 sha256=3f370a2c5f95abe34e83850350b0e498e4f230523935056b1c23cc03abb846ed\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8ltuw32w/wheels/de/aa/c5/27b5682c5592b7c0eecc3e208f176dedf6b11a61cf2a910b85\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=d6c91c330e70f45d8afb40596b822ed5a88b3b16f97f37ef8214b072256b6fa0\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built fire llamafactory ffmpy\n",
            "Installing collected packages: pydub, ffmpy, xxhash, websockets, uvloop, ujson, tomlkit, shtab, semantic-version, ruff, rouge-chinese, requests, python-multipart, python-dotenv, pyarrow, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, httptools, h11, fire, einops, dnspython, dill, aiofiles, watchfiles, uvicorn, tiktoken, starlette, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, httpcore, email_validator, tyro, sse-starlette, nvidia-cusolver-cu12, httpx, gradio-client, fastapi-cli, datasets, fastapi, accelerate, trl, peft, gradio, llamafactory\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.32.0 aiofiles-23.2.1 datasets-2.20.0 dill-0.3.8 dnspython-2.6.1 einops-0.8.0 email_validator-2.2.0 fastapi-0.111.0 fastapi-cli-0.0.4 ffmpy-0.3.2 fire-0.6.0 gradio-4.37.2 gradio-client-1.0.2 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 llamafactory-0.8.3.dev0 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 orjson-3.10.6 peft-0.11.1 pyarrow-16.1.0 pydub-0.25.1 python-dotenv-1.0.1 python-multipart-0.0.9 requests-2.32.3 rouge-chinese-1.0.3 ruff-0.5.0 semantic-version-2.10.0 shtab-1.7.1 sse-starlette-2.1.2 starlette-0.37.2 tiktoken-0.7.0 tomlkit-0.12.0 trl-0.9.4 tyro-0.8.5 ujson-5.10.0 uvicorn-0.30.1 uvloop-0.19.0 watchfiles-0.22.0 websockets-11.0.3 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -r requirements.txt -q"
      ],
      "metadata": {
        "id": "cUPmar8o1QIM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml\n",
        "!llamafactory-cli chat examples/inference/llama3_lora_sft.yaml\n",
        "!llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQeUjTq9Ni04",
        "outputId": "0975b215-3871-4ea8-d69f-64ac033fd8f0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-04 06:04:06.178752: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-04 06:04:06.178820: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-04 06:04:06.307829: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-04 06:04:06.396511: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-04 06:04:07.673088: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/04/2024 06:04:15 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16\n",
            "[INFO|tokenization_auto.py:666] 2024-07-04 06:04:15,316 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1024, in raise_for_status\n",
            "    raise HTTPError(http_error_msg, response=self)\n",
            "requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 399, in cached_file\n",
            "    resolved_file = hf_hub_download(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1221, in hf_hub_download\n",
            "    return _hf_hub_download_to_cache_dir(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1325, in _hf_hub_download_to_cache_dir\n",
            "    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1823, in _raise_on_head_call_error\n",
            "    raise head_call_error\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1722, in _get_metadata_or_catch_error\n",
            "    metadata = get_hf_file_metadata(url=url, proxies=proxies, timeout=etag_timeout, headers=headers)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1645, in get_hf_file_metadata\n",
            "    r = _request_wrapper(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 372, in _request_wrapper\n",
            "    response = _request_wrapper(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 396, in _request_wrapper\n",
            "    hf_raise_for_status(response)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\", line 321, in hf_raise_for_status\n",
            "    raise GatedRepoError(message, response) from e\n",
            "huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-66863b5f-49e3d3d02881ac272130e9fd;574de1dc-91ed-4f54-b310-435a952e94cf)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
            "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/cli.py\", line 111, in main\n",
            "    run_exp()\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 50, in run_exp\n",
            "    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 44, in run_sft\n",
            "    tokenizer_module = load_tokenizer(model_args)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/model/loader.py\", line 69, in load_tokenizer\n",
            "    tokenizer = AutoTokenizer.from_pretrained(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\", line 837, in from_pretrained\n",
            "    config = AutoConfig.from_pretrained(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\", line 934, in from_pretrained\n",
            "    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 632, in get_config_dict\n",
            "    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 689, in _get_config_dict\n",
            "    resolved_config_file = cached_file(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 417, in cached_file\n",
            "    raise EnvironmentError(\n",
            "OSError: You are trying to access a gated repo.\n",
            "Make sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct.\n",
            "401 Client Error. (Request ID: Root=1-66863b5f-49e3d3d02881ac272130e9fd;574de1dc-91ed-4f54-b310-435a952e94cf)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
            "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it.\n",
            "2024-07-04 06:04:20.494043: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-04 06:04:20.494102: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-04 06:04:20.495551: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-04 06:04:20.503065: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-04 06:04:21.835853: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[INFO|tokenization_auto.py:666] 2024-07-04 06:04:28,751 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1024, in raise_for_status\n",
            "    raise HTTPError(http_error_msg, response=self)\n",
            "requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 399, in cached_file\n",
            "    resolved_file = hf_hub_download(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1221, in hf_hub_download\n",
            "    return _hf_hub_download_to_cache_dir(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1325, in _hf_hub_download_to_cache_dir\n",
            "    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1823, in _raise_on_head_call_error\n",
            "    raise head_call_error\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1722, in _get_metadata_or_catch_error\n",
            "    metadata = get_hf_file_metadata(url=url, proxies=proxies, timeout=etag_timeout, headers=headers)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1645, in get_hf_file_metadata\n",
            "    r = _request_wrapper(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 372, in _request_wrapper\n",
            "    response = _request_wrapper(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 396, in _request_wrapper\n",
            "    hf_raise_for_status(response)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\", line 321, in hf_raise_for_status\n",
            "    raise GatedRepoError(message, response) from e\n",
            "huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-66863b6c-43c2faa36b004c7c305057f1;ff177dbd-81ea-48b6-b262-e06267c22dcc)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
            "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/cli.py\", line 81, in main\n",
            "    run_chat()\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/chat/chat_model.py\", line 126, in run_chat\n",
            "    chat_model = ChatModel()\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/chat/chat_model.py\", line 43, in __init__\n",
            "    self.engine: \"BaseEngine\" = HuggingfaceEngine(model_args, data_args, finetuning_args, generating_args)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/chat/hf_engine.py\", line 53, in __init__\n",
            "    tokenizer_module = load_tokenizer(model_args)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/model/loader.py\", line 69, in load_tokenizer\n",
            "    tokenizer = AutoTokenizer.from_pretrained(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\", line 837, in from_pretrained\n",
            "    config = AutoConfig.from_pretrained(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\", line 934, in from_pretrained\n",
            "    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 632, in get_config_dict\n",
            "    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 689, in _get_config_dict\n",
            "    resolved_config_file = cached_file(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 417, in cached_file\n",
            "    raise EnvironmentError(\n",
            "OSError: You are trying to access a gated repo.\n",
            "Make sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct.\n",
            "401 Client Error. (Request ID: Root=1-66863b6c-43c2faa36b004c7c305057f1;ff177dbd-81ea-48b6-b262-e06267c22dcc)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
            "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it.\n",
            "2024-07-04 06:04:33.628834: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-04 06:04:33.628888: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-04 06:04:33.630273: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-04 06:04:33.637834: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-04 06:04:34.924957: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[INFO|tokenization_auto.py:666] 2024-07-04 06:04:41,954 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1024, in raise_for_status\n",
            "    raise HTTPError(http_error_msg, response=self)\n",
            "requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 399, in cached_file\n",
            "    resolved_file = hf_hub_download(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1221, in hf_hub_download\n",
            "    return _hf_hub_download_to_cache_dir(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1325, in _hf_hub_download_to_cache_dir\n",
            "    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1823, in _raise_on_head_call_error\n",
            "    raise head_call_error\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1722, in _get_metadata_or_catch_error\n",
            "    metadata = get_hf_file_metadata(url=url, proxies=proxies, timeout=etag_timeout, headers=headers)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1645, in get_hf_file_metadata\n",
            "    r = _request_wrapper(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 372, in _request_wrapper\n",
            "    response = _request_wrapper(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 396, in _request_wrapper\n",
            "    hf_raise_for_status(response)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\", line 321, in hf_raise_for_status\n",
            "    raise GatedRepoError(message, response) from e\n",
            "huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-66863b79-0d7dedd51e418c044f3ae429;0ef626f7-49df-4de0-8b58-e3cd5fabb349)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
            "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/cli.py\", line 87, in main\n",
            "    export_model()\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 72, in export_model\n",
            "    tokenizer_module = load_tokenizer(model_args)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/model/loader.py\", line 69, in load_tokenizer\n",
            "    tokenizer = AutoTokenizer.from_pretrained(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\", line 837, in from_pretrained\n",
            "    config = AutoConfig.from_pretrained(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\", line 934, in from_pretrained\n",
            "    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 632, in get_config_dict\n",
            "    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 689, in _get_config_dict\n",
            "    resolved_config_file = cached_file(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 417, in cached_file\n",
            "    raise EnvironmentError(\n",
            "OSError: You are trying to access a gated repo.\n",
            "Make sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct.\n",
            "401 Client Error. (Request ID: Root=1-66863b79-0d7dedd51e418c044f3ae429;0ef626f7-49df-4de0-8b58-e3cd5fabb349)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
            "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !llamafactory-cli webui"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJyG5-kPNs8H",
        "outputId": "bf7edd32-0e89-4ee6-f0ab-66e50176b5d0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-04 06:04:47.779344: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-04 06:04:47.779401: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-04 06:04:47.781379: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-04 06:04:47.792543: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-04 06:04:49.491454: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Running on local URL:  http://0.0.0.0:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2664, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/cli.py\", line 115, in main\n",
            "    run_web_ui()\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/webui/interface.py\", line 90, in run_web_ui\n",
            "    create_ui().queue().launch(share=gradio_share, server_name=server_name, inbrowser=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2569, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2668, in block_thread\n",
            "    self.server.close()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/http_server.py\", line 68, in close\n",
            "    self.thread.join(timeout=5)\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1100, in join\n",
            "    self._wait_for_tstate_lock(timeout=max(timeout, 0))\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python==0.2.24\n",
        "# !pip install llama-index==0.9.19"
      ],
      "metadata": {
        "id": "ssWdwX-rFGiL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install bitsandbytes -q"
      ],
      "metadata": {
        "id": "12XMCj1Z1RuE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "src > llamafactory > extras > constants.py\n",
        "\n",
        "register_model_group(\n",
        "    models={\n",
        "        \"llama-3-typhoon-v1.5-8b-instruct\": {\n",
        "            DownloadSource.DEFAULT: \"scb10x/llama-3-typhoon-v1.5-8b-instruct\",\n",
        "            # DownloadSource.MODELSCOPE: \"ZhipuAI/chatglm3-6b-base\",\n",
        "        },\n",
        "        # \"ChatGLM3-6B-Chat\": {\n",
        "        #     DownloadSource.DEFAULT: \"THUDM/chatglm3-6b\",\n",
        "        #     DownloadSource.MODELSCOPE: \"ZhipuAI/chatglm3-6b\",\n",
        "        # },\n",
        "    },\n",
        "    template=\"llama3\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "GgJ9dFA3ELLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "\n",
        "# print(torch.cuda.is_available())\n",
        "# print(torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "id": "71pV7BSCHEsB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !nvcc --version"
      ],
      "metadata": {
        "id": "dimR0RFVG9BI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !nvidia-smi"
      ],
      "metadata": {
        "id": "Y6O7rCKBFvvv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SRC > webui.py\n",
        "\n",
        "def main():\n",
        "    gradio_share = os.environ.get(\"GRADIO_SHARE\", \"0\").lower() in [\"true\", \"1\"]\n",
        "    server_name = os.environ.get(\"GRADIO_SERVER_NAME\", \"0.0.0.0\")\n",
        "    create_ui().queue().launch(share=gradio_share, server_name=server_name, inbrowser=True)\n",
        "    # create_ui().queue().launch(share=True, server_name=server_name, inbrowser=True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "raBCkKvLHPGQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ",\n",
        " \"WangchanThaiInstruct_7.24\" : {\n",
        "        \"hf_hub_url\":\"airesearch/WangchanThaiInstruct_7.24\",\n",
        "        \"columns\": {\n",
        "            \"prompt\":\"Instruction\",\n",
        "            \"query\":\"Input\",\n",
        "            \"response\":\"Output\"\n",
        "        }\n",
        "  },"
      ],
      "metadata": {
        "id": "P85JSUcDJlnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bitsandbytes>=0.39.0"
      ],
      "metadata": {
        "id": "3PwLt9JfHOVx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nZmu--V4WkFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!CUDA_VISIBLE_DEVICES=0 python src/webui.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5owsv_sV3m46",
        "outputId": "0b0248c8-6fc5-438a-c604-743f4e788971"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-04 06:48:38.433894: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-04 06:48:38.433953: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-04 06:48:38.435540: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-04 06:48:38.443753: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-04 06:48:39.752869: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Running on local URL:  http://0.0.0.0:7860\n",
            "Running on public URL: https://cb8860d438e1bac287.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "2024-07-04 06:52:09.892550: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-04 06:52:09.892609: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-04 06:52:09.894710: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-04 06:52:11.786159: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/04/2024 06:52:18 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
            "07/04/2024 06:52:18 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-04 06:52:18,324 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--scb10x--llama-3-typhoon-v1.5-8b-instruct/snapshots/b97b72988589a285ce48bc0cf36976ff1dd39ef1/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-04 06:52:18,324 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-04 06:52:18,324 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--scb10x--llama-3-typhoon-v1.5-8b-instruct/snapshots/b97b72988589a285ce48bc0cf36976ff1dd39ef1/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-04 06:52:18,324 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--scb10x--llama-3-typhoon-v1.5-8b-instruct/snapshots/b97b72988589a285ce48bc0cf36976ff1dd39ef1/tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-07-04 06:52:18,774 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/04/2024 06:52:18 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
            "07/04/2024 06:52:18 - INFO - llamafactory.data.loader - Loading dataset airesearch/WangchanThaiInstruct_7.24...\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Converting format of dataset (num_proc=16): 100% 200/200 [00:00<00:00, 300.48 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 200/200 [00:16<00:00, 11.90 examples/s]\n",
            "input_ids:\n",
            "[128000, 128006, 882, 128007, 271, 122916, 111931, 48742, 114285, 100383, 101, 123582, 26265, 102624, 100630, 48271, 55784, 101880, 20795, 101039, 102286, 105208, 100879, 107692, 103267, 107093, 119248, 101324, 100427, 100301, 108795, 94974, 85894, 103789, 103207, 104045, 109008, 119075, 94974, 102935, 86145, 103257, 101652, 107511, 40955, 101131, 128009, 128006, 78191, 128007, 271, 103567, 76841, 48742, 23780, 101499, 26265, 73367, 100553, 42686, 106257, 38133, 59095, 104695, 20431, 100626, 20431, 105819, 122719, 38313, 65841, 100824, 29419, 111931, 48742, 102822, 100629, 101324, 220, 18, 103021, 94974, 104695, 118526, 19138, 1432, 16, 13, 30830, 25, 121692, 126874, 26265, 102624, 100618, 100630, 48271, 55784, 101880, 20795, 35609, 100535, 100741, 102444, 103513, 65841, 49220, 106131, 26265, 103789, 100301, 100877, 100863, 100629, 101324, 23084, 104125, 103881, 220, 17, 13, 22, 100568, 102235, 96575, 84681, 38313, 65841, 114074, 20431, 101606, 102286, 26265, 84646, 37242, 101290, 110512, 100427, 117157, 20795, 101008, 100568, 103061, 115102, 126874, 26265, 102624, 100618, 125236, 103584, 100488, 35609, 48742, 106257, 55784, 105587, 19138, 103513, 65841, 118432, 26265, 103789, 100301, 100877, 100863, 100629, 101324, 23084, 104125, 103881, 220, 18, 13, 22, 101111, 220, 15, 13, 21, 121946, 114317, 38133, 84646, 119700, 100561, 99030, 101540, 104125, 100418, 100974, 101648, 115772, 100590, 20795, 125500, 103176, 73367, 104367, 41427, 100863, 103863, 101339, 103207, 105607, 26265, 84646, 111041, 114945, 56870, 106841, 104422, 101290, 100561, 100764, 84681, 123300, 85894, 103789, 103207, 104045, 109008, 119075, 94974, 102935, 86145, 101039, 65784, 23780, 100694, 41427, 100297, 126874, 26265, 102624, 101279, 26265, 103789, 1432, 17, 13, 20163, 315, 17657, 25, 114288, 100974, 126023, 107012, 105175, 101652, 104422, 107781, 30830, 106561, 100630, 48271, 55784, 101880, 20795, 100869, 48271, 23084, 104125, 103881, 220, 17, 13, 16, 103455, 65784, 108125, 100736, 110856, 29419, 103257, 31534, 37242, 100694, 103828, 87448, 220, 2366, 16, 100869, 38313, 65841, 124782, 60984, 125644, 114821, 101127, 101131, 38313, 100736, 480, 15259, 320, 38, 2177, 5165, 33620, 8, 220, 102076, 101608, 31534, 102019, 94974, 101039, 101343, 27379, 55784, 110372, 101527, 23780, 65784, 20795, 105762, 100857, 101144, 31885, 108122, 100603, 20431, 121263, 126464, 103216, 100427, 40955, 127783, 101111, 108122, 100603, 20431, 101416, 104955, 105728, 105415, 127858, 101521, 36984, 103216, 121263, 41427, 59095, 102076, 101608, 114499, 114368, 122637, 100517, 32882, 37242, 101416, 115832, 48271, 100630, 48271, 55784, 101880, 20795, 100534, 115961, 100740, 105523, 101041, 114281, 108676, 59095, 100301, 100658, 41427, 100658, 111007, 122248, 101608, 114499, 106662, 100764, 100678, 105248, 48939, 102877, 20795, 101142, 65784, 114575, 100863, 100534, 100517, 102935, 86145, 118486, 105910, 38133, 123314, 56870, 114288, 100974, 117498, 65841, 102935, 86145, 28991, 104298, 101339, 49220, 104417, 100626, 20431, 116596, 100935, 103863, 36748, 108676, 59095, 100488, 101540, 103691, 100824, 106662, 109411, 100427, 105208, 104191, 100353, 102110, 36984, 48271, 96575, 42686, 101304, 31534, 100869, 38313, 65841, 87132, 109001, 120110, 104417, 38133, 101290, 94974, 105523, 101041, 59095, 102584, 108380, 100957, 114285, 101648, 115772, 100590, 20795, 94974, 85894, 103789, 103207, 104045, 109008, 119075, 94974, 102935, 86145, 40955, 100972, 27379, 101286, 104417, 49220, 38133, 101840, 35609, 117118, 103061, 94974, 105523, 101041, 107909, 48271, 36748, 105248, 48939, 102877, 106056, 102220, 1432, 18, 13, 763, 65249, 25, 114288, 100974, 126023, 107012, 21437, 102935, 86145, 113402, 52752, 100630, 48271, 55784, 101880, 20795, 103257, 36984, 48271, 87132, 19138, 108988, 108760, 118531, 102142, 100879, 103394, 101142, 121053, 220, 2366, 17, 109064, 100898, 124225, 111567, 104125, 103881, 220, 17, 13, 16, 100869, 38313, 65841, 101142, 65784, 100480, 76169, 55784, 110372, 101527, 23084, 109411, 101416, 38133, 101290, 104802, 100535, 101232, 103863, 101279, 121263, 41427, 100863, 102935, 86145, 113402, 52752, 120681, 100414, 105012, 38313, 126023, 107012, 21437, 102935, 86145, 113402, 52752, 38133, 122979, 104646, 100427, 100879, 103394, 101142, 121053, 220, 2366, 17, 109064, 106158, 84681, 23084, 104125, 103881, 220, 15, 13, 23, 102142, 127705, 48271, 114609, 100630, 48271, 55784, 101880, 20795, 100603, 20431, 100694, 103828, 87448, 94974, 100857, 107012, 21437, 102935, 86145, 113402, 52752, 100630, 48271, 55784, 101880, 20795, 103257, 36984, 48271, 87132, 19138, 108988, 108760, 118531, 110856, 29419, 220, 2366, 17, 101395, 65841, 100898, 124225, 111567, 104125, 103881, 220, 16, 13, 24, 100869, 38313, 65841, 110856, 29419, 220, 2366, 18, 109064, 106158, 84681, 101039, 102286, 23084, 104125, 103881, 220, 16, 13, 16, 102076, 101608, 31534, 23780, 107012, 21437, 102935, 86145, 113402, 52752, 36984, 48271, 100629, 101324, 100427, 107259, 84646, 101041, 59095, 100534, 59095, 107017, 123300, 101339, 114074, 20431, 101606, 102286, 107259, 111567, 104125, 103881, 220, 17, 103455, 65784, 114575, 100863, 103257, 125983, 48271, 35609, 125987, 38313, 24152, 105012, 61516, 109313, 126874, 26265, 102624, 100922, 101775, 110225, 20431, 111041, 114945, 56870, 106841, 108532, 48271, 55784, 101880, 20795, 123300, 101339, 113635, 122193, 85894, 103789, 103207, 104045, 109008, 119075, 94974, 102935, 86145, 40955, 120880, 23780, 102737, 271, 20795, 101547, 124642, 84681, 101028, 112849, 103975, 40955, 106819, 20795, 85894, 101802, 20795, 109008, 119075, 94974, 102935, 86145, 101279, 23780, 102737, 60984, 100547, 105587, 19138, 128009]\n",
            "inputs:\n",
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "เพราะเหตุใด เศรษฐกิจญี่ปุ่นเพียงประเทศเดียว ยังคงอยู่ในโหมดการผ่อนคลายนโยบายการเงินแบบเต็มตัว<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "ฮารุอิโกะ คูโรด้า ได้ให้คำตอบว่า มีเหตุผลอยู่ 3 ประการ ได้แก่\n",
            "\n",
            "\n",
            "1. GDP: ขนาดเศรษฐกิจของญี่ปุ่นยังเล็กกว่าช่วงก่อนโควิดอยู่ร้อยละ 2.7 สภาพถือว่าใกล้เคียงกับบ้านเราในขณะนี้ ส่วนขนาดเศรษฐกิจของสหรัฐและยุโรปใหญ่กว่าตอนก่อนโควิดอยู่ร้อยละ 3.7 และ 0.6 ตามลำดับ ไปเรียบร้อยแล้ว ดังนั้น ตรรกะในการคิดออกจะคล้ายกับแบงก์ชาติบ้านเราคือควรผ่อนคลายนโยบายการเงินเพื่อประคองเศรษฐกิจไปก่อน\n",
            "\n",
            "\n",
            "2. Terms of Trade: แม้ว่าอัตราการเติบโต GDP ของญี่ปุ่น ที่ร้อยละ 2.1 เมื่อวัดในปีแบบงบประมาณ 2021 ทว่าหากพิจารณาจาก ตัววัด GNI (Gross National Income)  ซึ่งทำการเพิ่มปัจจัยอื่น ๆ อาทิ รายได้จากการลงทุนในต่างประเทศ และ รายได้จากกำไร หรือขาดทุนจากการค้า ซึ่งมีค่าเป็นลบจากการที่ญี่ปุ่นเป็นผู้นำเข้าสินค้าโภคภัณฑ์ซึ่งมีราคาสูงขึ้นเมื่อคิดเป็นเงินสกุลดอลลาร์ แม้ว่าค่าเงินเยนจะช่วยให้การส่งออกสินค้าและบริการ มีราคาในประเทศปลายทางที่ถูกลง ทว่าไม่ได้ช่วยด้านการนำเข้าแต่อย่างใด ดังนั้นการผ่อนคลายนโยบายการเงินต้องมาช่วยชดเชยในส่วนการนำเข้าที่สูงขึ้นด้วย\n",
            "\n",
            "\n",
            "3. Inflation: แม้ว่าอัตราเงินเฟ้อญี่ปุ่นแบบที่ไม่รวมอาหารสด ในเดือนเมษายน 2022 จะเท่ากับร้อยละ 2.1 ทว่าเมื่อนำปัจจัยราคาจากด้านพลังงานออกไปจากการคิดเงินเฟ้อ ปรากฏว่าอัตราเงินเฟ้อดังกล่าวในเดือนเมษายน 2022 จะเหลือร้อยละ 0.8 ในขณะที่ทางการญี่ปุ่นได้ประมาณการ อัตราเงินเฟ้อญี่ปุ่นแบบที่ไม่รวมอาหารสดในปี 2022 ว่าเท่ากับร้อยละ 1.9 ทว่าในปี 2023 จะเหลือเพียงร้อยละ 1.1 ซึ่งอัตราเงินเฟ้อที่อยู่ในระดับเข้าเป้าหมายควรจะใกล้เคียงระดับร้อยละ 2 เมื่อคิดแบบเฉลี่ยตลอดวัฏจักรเศรษฐกิจ จึงทำให้แบงก์ชาติญี่ปุ่นควรจะดำเนินการผ่อนคลายนโยบายการเงินต่อไปอีก\n",
            "\n",
            "นั่นคือ น่าจะกระตุ้นผ่านนโยบายการเงินไปอีกพักใหญ่<|eot_id|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 103567, 76841, 48742, 23780, 101499, 26265, 73367, 100553, 42686, 106257, 38133, 59095, 104695, 20431, 100626, 20431, 105819, 122719, 38313, 65841, 100824, 29419, 111931, 48742, 102822, 100629, 101324, 220, 18, 103021, 94974, 104695, 118526, 19138, 1432, 16, 13, 30830, 25, 121692, 126874, 26265, 102624, 100618, 100630, 48271, 55784, 101880, 20795, 35609, 100535, 100741, 102444, 103513, 65841, 49220, 106131, 26265, 103789, 100301, 100877, 100863, 100629, 101324, 23084, 104125, 103881, 220, 17, 13, 22, 100568, 102235, 96575, 84681, 38313, 65841, 114074, 20431, 101606, 102286, 26265, 84646, 37242, 101290, 110512, 100427, 117157, 20795, 101008, 100568, 103061, 115102, 126874, 26265, 102624, 100618, 125236, 103584, 100488, 35609, 48742, 106257, 55784, 105587, 19138, 103513, 65841, 118432, 26265, 103789, 100301, 100877, 100863, 100629, 101324, 23084, 104125, 103881, 220, 18, 13, 22, 101111, 220, 15, 13, 21, 121946, 114317, 38133, 84646, 119700, 100561, 99030, 101540, 104125, 100418, 100974, 101648, 115772, 100590, 20795, 125500, 103176, 73367, 104367, 41427, 100863, 103863, 101339, 103207, 105607, 26265, 84646, 111041, 114945, 56870, 106841, 104422, 101290, 100561, 100764, 84681, 123300, 85894, 103789, 103207, 104045, 109008, 119075, 94974, 102935, 86145, 101039, 65784, 23780, 100694, 41427, 100297, 126874, 26265, 102624, 101279, 26265, 103789, 1432, 17, 13, 20163, 315, 17657, 25, 114288, 100974, 126023, 107012, 105175, 101652, 104422, 107781, 30830, 106561, 100630, 48271, 55784, 101880, 20795, 100869, 48271, 23084, 104125, 103881, 220, 17, 13, 16, 103455, 65784, 108125, 100736, 110856, 29419, 103257, 31534, 37242, 100694, 103828, 87448, 220, 2366, 16, 100869, 38313, 65841, 124782, 60984, 125644, 114821, 101127, 101131, 38313, 100736, 480, 15259, 320, 38, 2177, 5165, 33620, 8, 220, 102076, 101608, 31534, 102019, 94974, 101039, 101343, 27379, 55784, 110372, 101527, 23780, 65784, 20795, 105762, 100857, 101144, 31885, 108122, 100603, 20431, 121263, 126464, 103216, 100427, 40955, 127783, 101111, 108122, 100603, 20431, 101416, 104955, 105728, 105415, 127858, 101521, 36984, 103216, 121263, 41427, 59095, 102076, 101608, 114499, 114368, 122637, 100517, 32882, 37242, 101416, 115832, 48271, 100630, 48271, 55784, 101880, 20795, 100534, 115961, 100740, 105523, 101041, 114281, 108676, 59095, 100301, 100658, 41427, 100658, 111007, 122248, 101608, 114499, 106662, 100764, 100678, 105248, 48939, 102877, 20795, 101142, 65784, 114575, 100863, 100534, 100517, 102935, 86145, 118486, 105910, 38133, 123314, 56870, 114288, 100974, 117498, 65841, 102935, 86145, 28991, 104298, 101339, 49220, 104417, 100626, 20431, 116596, 100935, 103863, 36748, 108676, 59095, 100488, 101540, 103691, 100824, 106662, 109411, 100427, 105208, 104191, 100353, 102110, 36984, 48271, 96575, 42686, 101304, 31534, 100869, 38313, 65841, 87132, 109001, 120110, 104417, 38133, 101290, 94974, 105523, 101041, 59095, 102584, 108380, 100957, 114285, 101648, 115772, 100590, 20795, 94974, 85894, 103789, 103207, 104045, 109008, 119075, 94974, 102935, 86145, 40955, 100972, 27379, 101286, 104417, 49220, 38133, 101840, 35609, 117118, 103061, 94974, 105523, 101041, 107909, 48271, 36748, 105248, 48939, 102877, 106056, 102220, 1432, 18, 13, 763, 65249, 25, 114288, 100974, 126023, 107012, 21437, 102935, 86145, 113402, 52752, 100630, 48271, 55784, 101880, 20795, 103257, 36984, 48271, 87132, 19138, 108988, 108760, 118531, 102142, 100879, 103394, 101142, 121053, 220, 2366, 17, 109064, 100898, 124225, 111567, 104125, 103881, 220, 17, 13, 16, 100869, 38313, 65841, 101142, 65784, 100480, 76169, 55784, 110372, 101527, 23084, 109411, 101416, 38133, 101290, 104802, 100535, 101232, 103863, 101279, 121263, 41427, 100863, 102935, 86145, 113402, 52752, 120681, 100414, 105012, 38313, 126023, 107012, 21437, 102935, 86145, 113402, 52752, 38133, 122979, 104646, 100427, 100879, 103394, 101142, 121053, 220, 2366, 17, 109064, 106158, 84681, 23084, 104125, 103881, 220, 15, 13, 23, 102142, 127705, 48271, 114609, 100630, 48271, 55784, 101880, 20795, 100603, 20431, 100694, 103828, 87448, 94974, 100857, 107012, 21437, 102935, 86145, 113402, 52752, 100630, 48271, 55784, 101880, 20795, 103257, 36984, 48271, 87132, 19138, 108988, 108760, 118531, 110856, 29419, 220, 2366, 17, 101395, 65841, 100898, 124225, 111567, 104125, 103881, 220, 16, 13, 24, 100869, 38313, 65841, 110856, 29419, 220, 2366, 18, 109064, 106158, 84681, 101039, 102286, 23084, 104125, 103881, 220, 16, 13, 16, 102076, 101608, 31534, 23780, 107012, 21437, 102935, 86145, 113402, 52752, 36984, 48271, 100629, 101324, 100427, 107259, 84646, 101041, 59095, 100534, 59095, 107017, 123300, 101339, 114074, 20431, 101606, 102286, 107259, 111567, 104125, 103881, 220, 17, 103455, 65784, 114575, 100863, 103257, 125983, 48271, 35609, 125987, 38313, 24152, 105012, 61516, 109313, 126874, 26265, 102624, 100922, 101775, 110225, 20431, 111041, 114945, 56870, 106841, 108532, 48271, 55784, 101880, 20795, 123300, 101339, 113635, 122193, 85894, 103789, 103207, 104045, 109008, 119075, 94974, 102935, 86145, 40955, 120880, 23780, 102737, 271, 20795, 101547, 124642, 84681, 101028, 112849, 103975, 40955, 106819, 20795, 85894, 101802, 20795, 109008, 119075, 94974, 102935, 86145, 101279, 23780, 102737, 60984, 100547, 105587, 19138, 128009]\n",
            "labels:\n",
            "ฮารุอิโกะ คูโรด้า ได้ให้คำตอบว่า มีเหตุผลอยู่ 3 ประการ ได้แก่\n",
            "\n",
            "\n",
            "1. GDP: ขนาดเศรษฐกิจของญี่ปุ่นยังเล็กกว่าช่วงก่อนโควิดอยู่ร้อยละ 2.7 สภาพถือว่าใกล้เคียงกับบ้านเราในขณะนี้ ส่วนขนาดเศรษฐกิจของสหรัฐและยุโรปใหญ่กว่าตอนก่อนโควิดอยู่ร้อยละ 3.7 และ 0.6 ตามลำดับ ไปเรียบร้อยแล้ว ดังนั้น ตรรกะในการคิดออกจะคล้ายกับแบงก์ชาติบ้านเราคือควรผ่อนคลายนโยบายการเงินเพื่อประคองเศรษฐกิจไปก่อน\n",
            "\n",
            "\n",
            "2. Terms of Trade: แม้ว่าอัตราการเติบโต GDP ของญี่ปุ่น ที่ร้อยละ 2.1 เมื่อวัดในปีแบบงบประมาณ 2021 ทว่าหากพิจารณาจาก ตัววัด GNI (Gross National Income)  ซึ่งทำการเพิ่มปัจจัยอื่น ๆ อาทิ รายได้จากการลงทุนในต่างประเทศ และ รายได้จากกำไร หรือขาดทุนจากการค้า ซึ่งมีค่าเป็นลบจากการที่ญี่ปุ่นเป็นผู้นำเข้าสินค้าโภคภัณฑ์ซึ่งมีราคาสูงขึ้นเมื่อคิดเป็นเงินสกุลดอลลาร์ แม้ว่าค่าเงินเยนจะช่วยให้การส่งออกสินค้าและบริการ มีราคาในประเทศปลายทางที่ถูกลง ทว่าไม่ได้ช่วยด้านการนำเข้าแต่อย่างใด ดังนั้นการผ่อนคลายนโยบายการเงินต้องมาช่วยชดเชยในส่วนการนำเข้าที่สูงขึ้นด้วย\n",
            "\n",
            "\n",
            "3. Inflation: แม้ว่าอัตราเงินเฟ้อญี่ปุ่นแบบที่ไม่รวมอาหารสด ในเดือนเมษายน 2022 จะเท่ากับร้อยละ 2.1 ทว่าเมื่อนำปัจจัยราคาจากด้านพลังงานออกไปจากการคิดเงินเฟ้อ ปรากฏว่าอัตราเงินเฟ้อดังกล่าวในเดือนเมษายน 2022 จะเหลือร้อยละ 0.8 ในขณะที่ทางการญี่ปุ่นได้ประมาณการ อัตราเงินเฟ้อญี่ปุ่นแบบที่ไม่รวมอาหารสดในปี 2022 ว่าเท่ากับร้อยละ 1.9 ทว่าในปี 2023 จะเหลือเพียงร้อยละ 1.1 ซึ่งอัตราเงินเฟ้อที่อยู่ในระดับเข้าเป้าหมายควรจะใกล้เคียงระดับร้อยละ 2 เมื่อคิดแบบเฉลี่ยตลอดวัฏจักรเศรษฐกิจ จึงทำให้แบงก์ชาติญี่ปุ่นควรจะดำเนินการผ่อนคลายนโยบายการเงินต่อไปอีก\n",
            "\n",
            "นั่นคือ น่าจะกระตุ้นผ่านนโยบายการเงินไปอีกพักใหญ่<|eot_id|>\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:733] 2024-07-04 06:52:39,229 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--scb10x--llama-3-typhoon-v1.5-8b-instruct/snapshots/b97b72988589a285ce48bc0cf36976ff1dd39ef1/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-07-04 06:52:39,231 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"scb10x/llama-3-typhoon-v1.5-8b-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "07/04/2024 06:52:39 - WARNING - llamafactory.model.model_utils.attention - FlashAttention-2 is not installed.\n",
            "07/04/2024 06:52:39 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 4 bit with bitsandbytes.\n",
            "[INFO|modeling_utils.py:3474] 2024-07-04 06:52:39,378 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--scb10x--llama-3-typhoon-v1.5-8b-instruct/snapshots/b97b72988589a285ce48bc0cf36976ff1dd39ef1/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1519] 2024-07-04 06:52:39,382 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:962] 2024-07-04 06:52:39,385 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 4/4 [01:35<00:00, 23.87s/it]\n",
            "[INFO|modeling_utils.py:4280] 2024-07-04 06:54:15,480 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4288] 2024-07-04 06:54:15,480 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at scb10x/llama-3-typhoon-v1.5-8b-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:917] 2024-07-04 06:54:15,615 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--scb10x--llama-3-typhoon-v1.5-8b-instruct/snapshots/b97b72988589a285ce48bc0cf36976ff1dd39ef1/generation_config.json\n",
            "[INFO|configuration_utils.py:962] 2024-07-04 06:54:15,616 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128009\n",
            "  ],\n",
            "  \"max_length\": 4096,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "07/04/2024 06:57:50 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/04/2024 06:57:50 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "07/04/2024 06:57:50 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/04/2024 06:57:50 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "07/04/2024 06:57:50 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_proj,v_proj,k_proj,up_proj,down_proj,o_proj,q_proj\n",
            "07/04/2024 06:57:50 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n",
            "[INFO|trainer.py:641] 2024-07-04 06:57:50,843 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2078] 2024-07-04 06:57:52,187 >> ***** Running training *****\n",
            "[INFO|trainer.py:2079] 2024-07-04 06:57:52,187 >>   Num examples = 200\n",
            "[INFO|trainer.py:2080] 2024-07-04 06:57:52,187 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:2081] 2024-07-04 06:57:52,187 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:2084] 2024-07-04 06:57:52,187 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "[INFO|trainer.py:2085] 2024-07-04 06:57:52,187 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:2086] 2024-07-04 06:57:52,187 >>   Total optimization steps = 50\n",
            "[INFO|trainer.py:2087] 2024-07-04 06:57:52,192 >>   Number of trainable parameters = 20,971,520\n",
            "  2% 1/50 [01:04<52:59, 64.90s/it]Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 0.0.0.0:7860 <> https://cb8860d438e1bac287.gradio.live\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/cli.py\", line 111, in main\n",
            "    run_exp()\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 50, in run_exp\n",
            "    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 90, in run_sft\n",
            "    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1885, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2216, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3250, in training_step\n",
            "    self.accelerator.backward(loss)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 2151, in backward\n",
            "    loss.backward(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 525, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 267, in backward\n",
            "    _engine_run_backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 744, in _engine_run_backward\n",
            "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n",
            "  2% 1/50 [01:58<1:36:41, 118.39s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#backup folder ไปยัง github"
      ],
      "metadata": {
        "id": "e63R7-kjj7fT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYe_nXCBjhJl",
        "outputId": "a02968d6-73bb-48d0-b8c8-04913c0a3be1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.11).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"thodsapon.th@gmail.com\"\n",
        "!git config --global user.name \"lengtsp\""
      ],
      "metadata": {
        "id": "8UxokB4MkBMS"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://<TOKEN>@github.com/username/repository.git"
      ],
      "metadata": {
        "id": "pz926QqPmMGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2_v_uPflCfK",
        "outputId": "2f1e1a20-39af-4437-dc0d-d45e39fe1573"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/lengtsp/llamafactory_V2024-07-04_changeconfig.git"
      ],
      "metadata": {
        "id": "jc_qX7b3miQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r LLaMA-Factory/* llamafactory_V2024-07-04_changeconfig/"
      ],
      "metadata": {
        "id": "rf-cpawzmM1U"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iqpbo8J6kLfF",
        "outputId": "0099bc01-5343-446c-d20f-5b084977c467"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llamafactory_V2024-07-04_changeconfig'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (3/3), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd llamafactory_V2024-07-04_changeconfig\n",
        "!git add .\n",
        "!git commit -m \"Add your commit message here\"\n",
        "!git push origin main\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkVbeT0UlNEt",
        "outputId": "a8588c9d-f8ee-42f6-c5c7-059f9bbe8082"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llamafactory_V2024-07-04_changeconfig\n",
            "warning: adding embedded git repository: llamafactory_V2024-07-04_changeconfig\n",
            "\u001b[33mhint: You've added another git repository inside your current repository.\u001b[m\n",
            "\u001b[33mhint: Clones of the outer repository will not contain the contents of\u001b[m\n",
            "\u001b[33mhint: the embedded repository and will not know how to obtain it.\u001b[m\n",
            "\u001b[33mhint: If you meant to add a submodule, use:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit submodule add <url> llamafactory_V2024-07-04_changeconfig\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: If you added this path by mistake, you can remove it from the\u001b[m\n",
            "\u001b[33mhint: index with:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit rm --cached llamafactory_V2024-07-04_changeconfig\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: See \"git help submodule\" for more information.\u001b[m\n",
            "[main 62265c1] Add your commit message here\n",
            " 353 files changed, 76315 insertions(+), 1 deletion(-)\n",
            " create mode 100644 =0.39.0\n",
            " create mode 100644 CITATION.cff\n",
            " create mode 100644 LICENSE\n",
            " create mode 100644 MANIFEST.in\n",
            " create mode 100644 Makefile\n",
            " rewrite README.md (100%)\n",
            " create mode 100644 README_zh.md\n",
            " create mode 100644 assets/benchmark.svg\n",
            " create mode 100644 assets/logo.png\n",
            " create mode 100644 assets/wechat.jpg\n",
            " create mode 100644 assets/wechat_npu.jpg\n",
            " create mode 100644 cache/ds_z2_config.json\n",
            " create mode 100644 cache/ds_z2_offload_config.json\n",
            " create mode 100644 cache/ds_z3_config.json\n",
            " create mode 100644 cache/ds_z3_offload_config.json\n",
            " create mode 100644 cache/user_config.yaml\n",
            " create mode 100644 config/2024-07-04-06-09-18.yaml\n",
            " create mode 100644 data/README.md\n",
            " create mode 100644 data/README_zh.md\n",
            " create mode 100644 data/alpaca_en_demo.json\n",
            " create mode 100644 data/alpaca_zh_demo.json\n",
            " create mode 100644 data/belle_multiturn/belle_multiturn.py\n",
            " create mode 100644 data/c4_demo.json\n",
            " create mode 100644 data/dataset_info.json\n",
            " create mode 100644 data/dpo_en_demo.json\n",
            " create mode 100644 data/dpo_zh_demo.json\n",
            " create mode 100644 data/glaive_toolcall_en_demo.json\n",
            " create mode 100644 data/glaive_toolcall_zh_demo.json\n",
            " create mode 100644 data/hh_rlhf_en/hh_rlhf_en.py\n",
            " create mode 100644 data/identity.json\n",
            " create mode 100644 data/kto_en_demo.json\n",
            " create mode 100644 data/mllm_demo.json\n",
            " create mode 100644 data/mllm_demo_data/1.jpg\n",
            " create mode 100644 data/mllm_demo_data/2.jpg\n",
            " create mode 100644 data/mllm_demo_data/3.jpg\n",
            " create mode 100644 data/ultra_chat/ultra_chat.py\n",
            " create mode 100644 data/wiki_demo.txt\n",
            " create mode 100644 docker/docker-cuda/Dockerfile\n",
            " create mode 100644 docker/docker-cuda/docker-compose.yml\n",
            " create mode 100644 docker/docker-npu/Dockerfile\n",
            " create mode 100644 docker/docker-npu/docker-compose.yml\n",
            " create mode 100644 evaluation/ceval/ceval.py\n",
            " create mode 100644 evaluation/ceval/ceval.zip\n",
            " create mode 100644 evaluation/ceval/mapping.json\n",
            " create mode 100644 evaluation/cmmlu/cmmlu.py\n",
            " create mode 100644 evaluation/cmmlu/cmmlu.zip\n",
            " create mode 100644 evaluation/cmmlu/mapping.json\n",
            " create mode 100644 evaluation/mmlu/mapping.json\n",
            " create mode 100644 evaluation/mmlu/mmlu.py\n",
            " create mode 100644 evaluation/mmlu/mmlu.zip\n",
            " create mode 100644 examples/README.md\n",
            " create mode 100644 examples/README_zh.md\n",
            " create mode 100644 examples/accelerate/fsdp_config.yaml\n",
            " create mode 100644 examples/deepspeed/ds_z0_config.json\n",
            " create mode 100644 examples/deepspeed/ds_z2_config.json\n",
            " create mode 100644 examples/deepspeed/ds_z2_offload_config.json\n",
            " create mode 100644 examples/deepspeed/ds_z3_config.json\n",
            " create mode 100644 examples/deepspeed/ds_z3_offload_config.json\n",
            " create mode 100644 examples/extras/badam/llama3_full_sft.yaml\n",
            " create mode 100644 examples/extras/badam/llama3_full_sft_ds3.yaml\n",
            " create mode 100644 examples/extras/fsdp_qlora/llama3_lora_sft.yaml\n",
            " create mode 100644 examples/extras/fsdp_qlora/train.sh\n",
            " create mode 100644 examples/extras/galore/llama3_full_sft.yaml\n",
            " create mode 100644 examples/extras/llama_pro/expand.sh\n",
            " create mode 100644 examples/extras/llama_pro/llama3_freeze_sft.yaml\n",
            " create mode 100644 examples/extras/loraplus/llama3_lora_sft.yaml\n",
            " create mode 100644 examples/extras/mod/llama3_full_sft.yaml\n",
            " create mode 100644 examples/extras/pissa/llama3_lora_sft.yaml\n",
            " create mode 100644 examples/inference/llama3.yaml\n",
            " create mode 100644 examples/inference/llama3_lora_sft.yaml\n",
            " create mode 100644 examples/inference/llama3_vllm.yaml\n",
            " create mode 100644 examples/merge_lora/llama3_gptq.yaml\n",
            " create mode 100644 examples/merge_lora/llama3_lora_sft.yaml\n",
            " create mode 100644 examples/train_full/llama3_full_predict.yaml\n",
            " create mode 100644 examples/train_full/llama3_full_sft_ds3.yaml\n",
            " create mode 100644 examples/train_lora/llama3_lora_dpo.yaml\n",
            " create mode 100644 examples/train_lora/llama3_lora_eval.yaml\n",
            " create mode 100644 examples/train_lora/llama3_lora_kto.yaml\n",
            " create mode 100644 examples/train_lora/llama3_lora_ppo.yaml\n",
            " create mode 100644 examples/train_lora/llama3_lora_predict.yaml\n",
            " create mode 100644 examples/train_lora/llama3_lora_pretrain.yaml\n",
            " create mode 100644 examples/train_lora/llama3_lora_reward.yaml\n",
            " create mode 100644 examples/train_lora/llama3_lora_sft.yaml\n",
            " create mode 100644 examples/train_lora/llama3_lora_sft_ds0.yaml\n",
            " create mode 100644 examples/train_lora/llama3_lora_sft_ds3.yaml\n",
            " create mode 100644 examples/train_lora/llama3_preprocess.yaml\n",
            " create mode 100644 examples/train_lora/llava1_5_lora_sft.yaml\n",
            " create mode 100644 examples/train_qlora/llama3_lora_sft_aqlm.yaml\n",
            " create mode 100644 examples/train_qlora/llama3_lora_sft_awq.yaml\n",
            " create mode 100644 examples/train_qlora/llama3_lora_sft_gptq.yaml\n",
            " create mode 100644 examples/train_qlora/llama3_lora_sft_otfq.yaml\n",
            " create mode 160000 llamafactory_V2024-07-04_changeconfig\n",
            " create mode 100644 pyproject.toml\n",
            " create mode 100644 requirements.txt\n",
            " create mode 100644 saves/llama-3-typhoon-v1.5-8b-instruct/lora/train_2024-07-04-06-09-18/llamaboard_config.yaml\n",
            " create mode 100644 saves/llama-3-typhoon-v1.5-8b-instruct/lora/train_2024-07-04-06-09-18/running_log.txt\n",
            " create mode 100644 saves/llama-3-typhoon-v1.5-8b-instruct/lora/train_2024-07-04-06-09-18/training_args.yaml\n",
            " create mode 100644 saves/llama-3-typhoon-v1.5-8b-instruct/lora/train_2024-07-04-06-29-46/llamaboard_config.yaml\n",
            " create mode 100644 saves/llama-3-typhoon-v1.5-8b-instruct/lora/train_2024-07-04-06-29-46/running_log.txt\n",
            " create mode 100644 saves/llama-3-typhoon-v1.5-8b-instruct/lora/train_2024-07-04-06-29-46/training_args.yaml\n",
            " create mode 100644 saves/llama-3-typhoon-v1.5-8b-instruct/lora/train_2024-07-04-06-50-43/llamaboard_config.yaml\n",
            " create mode 100644 saves/llama-3-typhoon-v1.5-8b-instruct/lora/train_2024-07-04-06-50-43/running_log.txt\n",
            " create mode 100644 saves/llama-3-typhoon-v1.5-8b-instruct/lora/train_2024-07-04-06-50-43/training_args.yaml\n",
            " create mode 100644 scripts/cal_flops.py\n",
            " create mode 100644 scripts/cal_lr.py\n",
            " create mode 100644 scripts/cal_ppl.py\n",
            " create mode 100644 scripts/length_cdf.py\n",
            " create mode 100644 scripts/llama_pro.py\n",
            " create mode 100644 scripts/llamafy_baichuan2.py\n",
            " create mode 100644 scripts/llamafy_qwen.py\n",
            " create mode 100644 scripts/loftq_init.py\n",
            " create mode 100644 scripts/pissa_init.py\n",
            " create mode 100644 scripts/test_toolcall.py\n",
            " create mode 100644 setup.py\n",
            " create mode 100644 src/api.py\n",
            " create mode 100644 src/llamafactory.egg-info/PKG-INFO\n",
            " create mode 100644 src/llamafactory.egg-info/SOURCES.txt\n",
            " create mode 100644 src/llamafactory.egg-info/dependency_links.txt\n",
            " create mode 100644 src/llamafactory.egg-info/entry_points.txt\n",
            " create mode 100644 src/llamafactory.egg-info/requires.txt\n",
            " create mode 100644 src/llamafactory.egg-info/top_level.txt\n",
            " create mode 100644 src/llamafactory/__init__.py\n",
            " create mode 100644 src/llamafactory/__pycache__/__init__.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/__pycache__/cli.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/__pycache__/launcher.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/api/__init__.py\n",
            " create mode 100644 src/llamafactory/api/__pycache__/__init__.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/api/__pycache__/app.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/api/__pycache__/chat.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/api/__pycache__/common.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/api/__pycache__/protocol.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/api/app.py\n",
            " create mode 100644 src/llamafactory/api/chat.py\n",
            " create mode 100644 src/llamafactory/api/common.py\n",
            " create mode 100644 src/llamafactory/api/protocol.py\n",
            " create mode 100644 src/llamafactory/chat/__init__.py\n",
            " create mode 100644 src/llamafactory/chat/__pycache__/__init__.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/chat/__pycache__/base_engine.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/chat/__pycache__/chat_model.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/chat/__pycache__/hf_engine.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/chat/__pycache__/vllm_engine.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/chat/base_engine.py\n",
            " create mode 100644 src/llamafactory/chat/chat_model.py\n",
            " create mode 100644 src/llamafactory/chat/hf_engine.py\n",
            " create mode 100644 src/llamafactory/chat/vllm_engine.py\n",
            " create mode 100644 src/llamafactory/cli.py\n",
            " create mode 100644 src/llamafactory/data/__init__.py\n",
            " create mode 100644 src/llamafactory/data/__pycache__/__init__.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/data/__pycache__/aligner.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/data/__pycache__/collator.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/data/__pycache__/data_utils.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/data/__pycache__/formatter.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/data/__pycache__/loader.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/data/__pycache__/parser.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/data/__pycache__/preprocess.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/data/__pycache__/template.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/data/__pycache__/tool_utils.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/data/aligner.py\n",
            " create mode 100644 src/llamafactory/data/collator.py\n",
            " create mode 100644 src/llamafactory/data/data_utils.py\n",
            " create mode 100644 src/llamafactory/data/formatter.py\n",
            " create mode 100644 src/llamafactory/data/loader.py\n",
            " create mode 100644 src/llamafactory/data/parser.py\n",
            " create mode 100644 src/llamafactory/data/preprocess.py\n",
            " create mode 100644 src/llamafactory/data/processors/__init__.py\n",
            " create mode 100644 src/llamafactory/data/processors/__pycache__/__init__.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/data/processors/__pycache__/feedback.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/data/processors/__pycache__/pairwise.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/data/processors/__pycache__/pretrain.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/data/processors/__pycache__/processor_utils.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/data/processors/__pycache__/supervised.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/data/processors/__pycache__/unsupervised.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/data/processors/feedback.py\n",
            " create mode 100644 src/llamafactory/data/processors/pairwise.py\n",
            " create mode 100644 src/llamafactory/data/processors/pretrain.py\n",
            " create mode 100644 src/llamafactory/data/processors/processor_utils.py\n",
            " create mode 100644 src/llamafactory/data/processors/supervised.py\n",
            " create mode 100644 src/llamafactory/data/processors/unsupervised.py\n",
            " create mode 100644 src/llamafactory/data/template.py\n",
            " create mode 100644 src/llamafactory/data/tool_utils.py\n",
            " create mode 100644 src/llamafactory/eval/__init__.py\n",
            " create mode 100644 src/llamafactory/eval/__pycache__/__init__.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/eval/__pycache__/evaluator.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/eval/__pycache__/template.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/eval/evaluator.py\n",
            " create mode 100644 src/llamafactory/eval/template.py\n",
            " create mode 100644 src/llamafactory/extras/__init__.py\n",
            " create mode 100644 src/llamafactory/extras/__pycache__/__init__.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/extras/__pycache__/constants.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/extras/__pycache__/env.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/extras/__pycache__/logging.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/extras/__pycache__/misc.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/extras/__pycache__/packages.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/extras/__pycache__/ploting.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/extras/constants.py\n",
            " create mode 100644 src/llamafactory/extras/env.py\n",
            " create mode 100644 src/llamafactory/extras/logging.py\n",
            " create mode 100644 src/llamafactory/extras/misc.py\n",
            " create mode 100644 src/llamafactory/extras/packages.py\n",
            " create mode 100644 src/llamafactory/extras/ploting.py\n",
            " create mode 100644 src/llamafactory/hparams/__init__.py\n",
            " create mode 100644 src/llamafactory/hparams/__pycache__/__init__.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/hparams/__pycache__/data_args.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/hparams/__pycache__/evaluation_args.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/hparams/__pycache__/finetuning_args.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/hparams/__pycache__/generating_args.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/hparams/__pycache__/model_args.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/hparams/__pycache__/parser.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/hparams/data_args.py\n",
            " create mode 100644 src/llamafactory/hparams/evaluation_args.py\n",
            " create mode 100644 src/llamafactory/hparams/finetuning_args.py\n",
            " create mode 100644 src/llamafactory/hparams/generating_args.py\n",
            " create mode 100644 src/llamafactory/hparams/model_args.py\n",
            " create mode 100644 src/llamafactory/hparams/parser.py\n",
            " create mode 100644 src/llamafactory/launcher.py\n",
            " create mode 100644 src/llamafactory/model/__init__.py\n",
            " create mode 100644 src/llamafactory/model/__pycache__/__init__.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/model/__pycache__/adapter.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/model/__pycache__/loader.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/model/__pycache__/patcher.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/model/adapter.py\n",
            " create mode 100644 src/llamafactory/model/loader.py\n",
            " create mode 100644 src/llamafactory/model/model_utils/__init__.py\n",
            " create mode 100644 src/llamafactory/model/model_utils/__pycache__/__init__.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/model/model_utils/__pycache__/attention.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/model/model_utils/__pycache__/checkpointing.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/model/model_utils/__pycache__/embedding.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/model/model_utils/__pycache__/longlora.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/model/model_utils/__pycache__/misc.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/model/model_utils/__pycache__/mod.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/model/model_utils/__pycache__/moe.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/model/model_utils/__pycache__/packing.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/model/model_utils/__pycache__/quantization.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/model/model_utils/__pycache__/rope.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/model/model_utils/__pycache__/unsloth.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/model/model_utils/__pycache__/valuehead.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/model/model_utils/__pycache__/visual.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/model/model_utils/attention.py\n",
            " create mode 100644 src/llamafactory/model/model_utils/checkpointing.py\n",
            " create mode 100644 src/llamafactory/model/model_utils/embedding.py\n",
            " create mode 100644 src/llamafactory/model/model_utils/longlora.py\n",
            " create mode 100644 src/llamafactory/model/model_utils/misc.py\n",
            " create mode 100644 src/llamafactory/model/model_utils/mod.py\n",
            " create mode 100644 src/llamafactory/model/model_utils/moe.py\n",
            " create mode 100644 src/llamafactory/model/model_utils/packing.py\n",
            " create mode 100644 src/llamafactory/model/model_utils/quantization.py\n",
            " create mode 100644 src/llamafactory/model/model_utils/rope.py\n",
            " create mode 100644 src/llamafactory/model/model_utils/unsloth.py\n",
            " create mode 100644 src/llamafactory/model/model_utils/valuehead.py\n",
            " create mode 100644 src/llamafactory/model/model_utils/visual.py\n",
            " create mode 100644 src/llamafactory/model/patcher.py\n",
            " create mode 100644 src/llamafactory/train/__init__.py\n",
            " create mode 100644 src/llamafactory/train/__pycache__/__init__.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/train/__pycache__/callbacks.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/train/__pycache__/trainer_utils.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/train/__pycache__/tuner.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/train/callbacks.py\n",
            " create mode 100644 src/llamafactory/train/dpo/__init__.py\n",
            " create mode 100644 src/llamafactory/train/dpo/__pycache__/__init__.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/train/dpo/__pycache__/trainer.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/train/dpo/__pycache__/workflow.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/train/dpo/trainer.py\n",
            " create mode 100644 src/llamafactory/train/dpo/workflow.py\n",
            " create mode 100644 src/llamafactory/train/kto/__init__.py\n",
            " create mode 100644 src/llamafactory/train/kto/__pycache__/__init__.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/train/kto/__pycache__/trainer.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/train/kto/__pycache__/workflow.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/train/kto/trainer.py\n",
            " create mode 100644 src/llamafactory/train/kto/workflow.py\n",
            " create mode 100644 src/llamafactory/train/ppo/__init__.py\n",
            " create mode 100644 src/llamafactory/train/ppo/__pycache__/__init__.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/train/ppo/__pycache__/ppo_utils.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/train/ppo/__pycache__/trainer.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/train/ppo/__pycache__/workflow.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/train/ppo/ppo_utils.py\n",
            " create mode 100644 src/llamafactory/train/ppo/trainer.py\n",
            " create mode 100644 src/llamafactory/train/ppo/workflow.py\n",
            " create mode 100644 src/llamafactory/train/pt/__init__.py\n",
            " create mode 100644 src/llamafactory/train/pt/__pycache__/__init__.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/train/pt/__pycache__/trainer.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/train/pt/__pycache__/workflow.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/train/pt/trainer.py\n",
            " create mode 100644 src/llamafactory/train/pt/workflow.py\n",
            " create mode 100644 src/llamafactory/train/rm/__init__.py\n",
            " create mode 100644 src/llamafactory/train/rm/__pycache__/__init__.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/train/rm/__pycache__/metric.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/train/rm/__pycache__/trainer.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/train/rm/__pycache__/workflow.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/train/rm/metric.py\n",
            " create mode 100644 src/llamafactory/train/rm/trainer.py\n",
            " create mode 100644 src/llamafactory/train/rm/workflow.py\n",
            " create mode 100644 src/llamafactory/train/sft/__init__.py\n",
            " create mode 100644 src/llamafactory/train/sft/__pycache__/__init__.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/train/sft/__pycache__/metric.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/train/sft/__pycache__/trainer.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/train/sft/__pycache__/workflow.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/train/sft/metric.py\n",
            " create mode 100644 src/llamafactory/train/sft/trainer.py\n",
            " create mode 100644 src/llamafactory/train/sft/workflow.py\n",
            " create mode 100644 src/llamafactory/train/trainer_utils.py\n",
            " create mode 100644 src/llamafactory/train/tuner.py\n",
            " create mode 100644 src/llamafactory/webui/__init__.py\n",
            " create mode 100644 src/llamafactory/webui/__pycache__/__init__.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/webui/__pycache__/chatter.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/webui/__pycache__/common.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/webui/__pycache__/css.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/webui/__pycache__/engine.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/webui/__pycache__/interface.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/webui/__pycache__/locales.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/webui/__pycache__/manager.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/webui/__pycache__/runner.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/webui/__pycache__/utils.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/webui/chatter.py\n",
            " create mode 100644 src/llamafactory/webui/common.py\n",
            " create mode 100644 src/llamafactory/webui/components/__init__.py\n",
            " create mode 100644 src/llamafactory/webui/components/__pycache__/__init__.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/webui/components/__pycache__/chatbot.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/webui/components/__pycache__/data.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/webui/components/__pycache__/eval.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/webui/components/__pycache__/export.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/webui/components/__pycache__/infer.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/webui/components/__pycache__/top.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/webui/components/__pycache__/train.cpython-310.pyc\n",
            " create mode 100644 src/llamafactory/webui/components/chatbot.py\n",
            " create mode 100644 src/llamafactory/webui/components/data.py\n",
            " create mode 100644 src/llamafactory/webui/components/eval.py\n",
            " create mode 100644 src/llamafactory/webui/components/export.py\n",
            " create mode 100644 src/llamafactory/webui/components/infer.py\n",
            " create mode 100644 src/llamafactory/webui/components/top.py\n",
            " create mode 100644 src/llamafactory/webui/components/train.py\n",
            " create mode 100644 src/llamafactory/webui/css.py\n",
            " create mode 100644 src/llamafactory/webui/engine.py\n",
            " create mode 100644 src/llamafactory/webui/interface.py\n",
            " create mode 100644 src/llamafactory/webui/locales.py\n",
            " create mode 100644 src/llamafactory/webui/manager.py\n",
            " create mode 100644 src/llamafactory/webui/runner.py\n",
            " create mode 100644 src/llamafactory/webui/utils.py\n",
            " create mode 100644 src/train.py\n",
            " create mode 100644 src/webui.py\n",
            " create mode 100644 tests/data/test_collator.py\n",
            " create mode 100644 tests/data/test_formatter.py\n",
            " create mode 100644 tests/data/test_processor.py\n",
            " create mode 100644 tests/data/test_supervised.py\n",
            " create mode 100644 tests/data/test_template.py\n",
            " create mode 100644 tests/eval/test_eval_template.py\n",
            " create mode 100644 tests/model/model_utils/test_attention.py\n",
            " create mode 100644 tests/model/model_utils/test_checkpointing.py\n",
            " create mode 100644 tests/model/model_utils/test_packing.py\n",
            " create mode 100644 tests/model/test_base.py\n",
            " create mode 100644 tests/model/test_freeze.py\n",
            " create mode 100644 tests/model/test_full.py\n",
            " create mode 100644 tests/model/test_lora.py\n",
            " create mode 100644 tests/model/test_pissa.py\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NwCVOwKnlQI3"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxCR69xxlSMk",
        "outputId": "84d9c344-f705-4f55-e8f5-ed74f49dfe11"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2ELokdSlT8v",
        "outputId": "fe5ea6fe-ce61-46ec-bedb-ccf23281c241"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    }
  ]
}